\section{System Development}\label{sec:development}
\phantomsection

\subsection{Implementation}
With a theoretical background and a system design, it is possible to present the implementation phase of the project. Roughly the system is decoupled into 2 independent parts. The first one represents the training of the system and the resulting model. The second one is the end-user application. Even though the two parts are independent from each other, they actually reuse the Digital Signal Processing module. Clearly both modules deal with processing raw WAV data.

The project with the codename ``Nosnore'' is written entirely in Python programming language, CPython interpreter version $2.7$. The main arguments to use Python as the main and only programming language are the following:

\begin{itemize}[topsep=5pt, partopsep=0pt,itemsep=3pt,parsep=1pt]
 \item since the project is released under open source MIT license, it is an advantage that Python itself is an open source project;
 \item nevertheless readability of a programming language is a disputable fact, generally Python syntax is considered well thought out, easy to read and maintain;
 \item Python is a very popular language between regular developers and researches, which diminished the gap of contributing to the project or reusing some of its functionality;
 \item it represents the perfect balance between a high and low level programming language, unlike many other languages like R, Julia or even Matlab/Octave although the scientific environments;
 \item acting as a wrapper to many Matlab and C subroutines that can be called from Python;
 \item developed standardized documentation system, which makes sharing and collaboration even easier;
 \item large variety of free libraries for scientific computations, like NumPy, SciPy, Matplotlib, Sklearn, etc;
 \item hierarchical module system;
 \item the standard library is mostly cross-platform code;
 \item Kivy is a cross-platform framework, which makes is possible to deploy Python application on Windows, OS X, Linux, Android and iOS.
\end{itemize}

As a conclusion Python is a complete programming solution and it was used both for scientific computations and for the high level application. Moreover since everything is build around Python, it is easy to transfer the functionality to a web server or any other extra system architecture layer.

The DSP module is build around existing functions with necessary adaptations. Since it doesn't involve a hierarchy or classification of object, it uses the procedural paradigm to construct algorithms. Nevertheless there is signal class presented in \ref{list1}, which encapsulated and performs basic functionality.

All the important computations are performed by a worker, which calls the DSP functions to perform the processing and get the features from the PSD. In \ref{list2} it takes each signal, split into smaller signals and applies the filtering, normalization and other methods. Eventually the valuable information represents the features, which are obviously named as ``features'' in the code.

\lstinputlisting[language=Python, caption={Worker Python code}, label=list2]{../sourcecode/worker.py}

Even though the autocorrelation can be done using the relation \eqref{eq:accor}, it requires more computational power and hence more time. Therefore for autocorrelation it is reasonable to do the inverse FFT to the product of the real FFT and the conjugate of the real FFT as presented in \ref{list3}.

\lstinputlisting[language=Python, caption={Autocorrelation function code}, label=list3]{../sourcecode/autocorrelation.py}

In figure \ref{fig:snorewave} is presented the snore wave in raw form as it is received from the input device. By following all the DSP steps described in \ref{subsubsec:dsp} the signal will result into a set of formants relevant for the study displayed in \ref{fig:snorepsd}. From this point, it can be seen that PSD holds some important information about the signal, particularly the peaks. The biggest spike happens around $150$ unit of frequency. Along with that some of the peaks are as well quite persistent for all the sample data -- close to $50$, $70$, $100$, $200$ and the neighborhood of $300$. 

Therefore the major task is to extract those peaks and their amplitude values. But the job of identified local maximas with no prior knowledge isn't trivial. At first it was attempted to detect the formants from the whole vector of data. In this case the formant was identified as such if it has a maximal value within a bin and is preceded to the left by a value lower than a specified $\delta$. The following method didn't work, because each vector was having a variable number of formants. There was no equivalent for the other signals to have or not have a feature at the same frequency as that signal had. Hence formants method was dropped, even though it identified correctly the formants, but couldn't guarantee to identify formants in the same frequency range for the other signals.

\begin{figure}[!ht]
\centering
  \includegraphics[width=15cm]{12_snorewave.eps}
\caption{Snore signal wave in time domain}
\label{fig:snorewave}
\end{figure}

\begin{figure}[!ht]
\centering
  \includegraphics[width=15cm]{13_snorepsd.eps}
\caption{Power Spectral Density of the snore signal}
\label{fig:snorepsd}
\end{figure}

That's why it was chosen to detect local maxima from predefined ranges of the signal vector. Clearly this is peaking on the data and searching for the pattern using human input. For this specific snoring case the following ranges were detected: $(45, 55), (80, 90), (95, 105), (145, 155), (195, 205),$ $ (245, 255), (300, 310)$. The algorithm how checking the maxima in those areas is shown in \ref{list4}. The visualization of the peaks in \ref{fig:peaks} clearly shows how $7$ local maxima were identified, which makes it $14$ features, because both the frequency and the magnitude values are stored.

\begin{figure}[!ht]
\centering
  \includegraphics[width=15cm]{14_peaks.eps}
\caption{The peaks in PSD of a snore signal}
\label{fig:peaks}
\end{figure}

This is only one set of all the features. To ensure oneself that there are no other peaks or excess of data in the frequency domain, it was decided to compute also the areas of the bins. That includes splitting up the signal into bins of a specific window size and computing the area using composite trapezoidal rule presented in \ref{list5}. Only the first $20$ bins are taking into consideration, since only the low frequencies matter in this case. In total there are $34$ features for each sample.

\lstinputlisting[language=Python, caption={Extract bin areas function}, label=list5]{../sourcecode/bins.py}

Before proceeding to training phase, the central question is where from to get the training data. For this project the data was kindly offered by Prague Psychiatric Center \cite{pcp} and it includes a $9$ hour recording of sleep sounds from a person diagnosed with Sleep Apnea. The snoring part estimates roughly to $6$ hours. The initial data was sampled at $44100$ Hz and resampled because of compression reasons to $11025$ Hz. The bottleneck here is the lack of snoring of a healthy person. In that way it would be possible to compare those two and identify the differences. 

Besides the positive examples fed to the classification algorithm, it is essential to acquire negative samples. Normally negative samples would include everything else. But it is hard to extract features from an infinite set of data. Therefore the infinity is shrinked to a set of most common noises during the night, like coughing, background noise from the streets, far airplane noises, sneezing, room silence. The negative samples were found from a free library of sounds \cite{freesounds}.

So by picking up positive and negative samples in total $5000$ samples each having $34$ features, the Logistic Regression model is build. The modeling is done using Sklean library that contains an interface to use Logistic Regression, therefore the code is pretty straight-forward shown in \ref{list6}. The coefficients of the training are stored in a JSON file for further predicting.

\lstinputlisting[language=Python, caption={Training using Logistic Regression}, label=list6]{../sourcecode/training.py}

This was the processing and training part described. Next is the end-user application build using Kivy framework, \cite{kivy}. Kivy is free and open source software. It runs on Android, iOS, Linux, OS X, and Windows. It is developed by the community along with the projects such as Python for Android, Kivy iOS. Kivy supports mouse, keyboard and multitouch platform specific events, contains a graphic library using OpenGL ES 2, has a range of Widgets. Kivy started as an initiative several years ago and becomes more and more popular among Python developers.

A simple application using Kivy was created to demonstrate how the detection works. The application was planned to be deployed mainly on Android platform, but unfortunately an important issue came out -- Kivy isn't yet having a stable module for signal input. The recommended way is using a recipe created by Kivy developers called ``audiostream'' library that deals with speakers and microphone on Android and iOS devices. Sadly the recipe is not guaranteed to work on any device and fails to capture the input from the microphone on popular Android phones.

\begin{figure}[!ht]
  \centering
  \subfloat[Sleep screen of the application\label{fig:screen1}]{%    
    \includegraphics[width=0.4\textwidth]{screen1.png}      
  }
  \vspace{0.08cm}
  \subfloat[Settings screen of the application\label{fig:screen2}]{%
    \includegraphics[width=0.4\textwidth]{screen2.png}
  }
  \caption{Application interface}
  \label{fig:screens}
\end{figure}

Hence the prototype works on desktop systems. The recommended way to use the application is to deploy it on a Raspberry Pi single-board computer and connect a USB microphone. The application has a simple interface and includes 3 screens. The sleep screen from figure \ref{fig:screen1} displays a single button to start and than stop the recording. The settings are set on figure \ref{fig:screen2} and it has the ability to set regular recording time.

The interface is using universal widgets and it's demanding any platform specific gestures -- everything can be done on any platform. The UI is written in kv language -- a special subset of Kivy for GUI building. A portion of the kv file is presented in \ref{list7}.

The recording made from start to the end is processed using the same DSP methods. After extracting the features from the signal portions those are classified using the predictions made by the model. The total time is computed and the time of snoring. This information is displayed right after the user clicks the stop button. As a conclusion this is the simple prototype build to demonstrate the functionality of the system.

\subsection{Testing and results}

As a rule of thumb in machine learning the acquired data should be split into 2 groups -- training and testing. Therefore $70\%$ of the data was used for training and $30\%$ for testing, i.e $3712$ samples for testing. The script that does the testing is listed in \ref{list8}. 

The coefficients are extracted from JSON file and the model is reconstructed from the trained state. The data is extracted from the database and sent for prediction. The prediction probabilities are also computed.

\lstinputlisting[language=Python, caption={Test training Python script}, label=list8]{../sourcecode/test_training.py}

And the results that is yielded are the following:

\begin{itemize}[topsep=5pt, partopsep=0pt,itemsep=3pt,parsep=1pt]
 \item $3499$ correctly labeled samples;
 \item $213$ incorrectly labeled samples;
 \item $94.2618534483\%$ accuracy. 
\end{itemize}

About $94\%$ of success in identifying snoring is a reasonable result. The percentage will probably drop for the training data out of a different set, which means a different input device. There haven't been done any real-world testing of the system in a home environment, therefore it is unknown how the system will behave by receiving input from a wide variety of microphones. That is one reason to make the application open for anybody who wants to use, test or improve the existing codebase. 

\clearpage